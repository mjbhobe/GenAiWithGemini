{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tutorial\n",
    "This workbook reproduces the tutorial on the Langchain website. We work with Google Gemini LLM and a local (Llama2) LLM served using Ollama. \n",
    "\n",
    "For instructions on setting up Llama2 using Ollama [online quickstart tutorial](https://python.langchain.com/docs/get_started/quickstart). For Google Gemini refer to [this link](https://ai.google.dev/tutorials/python_quickstart?_gl=1*ex542r*_up*MQ..&gclid=CjwKCAjw5ImwBhBtEiwAFHDZx4UgYI9p9vCREXwKpGPHjtBAcP4vFv3Wubx-27jkOcLhxCsUu4LSdBoC2QkQAvD_BwE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# to use the Google Gemini LLM\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is needed ONLY for properly formatting Gemini response\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace(\"â€¢\", \"  *\")\n",
    "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variables from .env file\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file with all keys\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of Gemini\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    # language model, for vision model use \"gemini-pro-vision\" instead\n",
    "    model=\"gemini-pro\",\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    temperature=0.6,\n",
    "    # gemini does not support system messages, this is a Langchain hack\n",
    "    convert_system_message_to_human=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **Automated Testing:**\n",
       "> \n",
       "> * **Unit Testing:** Langsmith can automatically generate unit tests for your code, ensuring that individual functions and methods work as expected.\n",
       "> * **Integration Testing:** Langsmith can create integration tests that verify the interactions between different modules or components of your application.\n",
       "> * **End-to-End Testing:** Langsmith can generate end-to-end tests that simulate real-world user scenarios, ensuring that the application functions correctly from start to finish.\n",
       "> \n",
       "> **Test Case Management:**\n",
       "> \n",
       "> * **Test Case Generation:** Langsmith can automatically generate test cases based on your code and requirements, reducing the time and effort required for manual test case creation.\n",
       "> * **Test Case Organization:** Langsmith provides a centralized platform for managing and organizing test cases, making it easy to track their status and progress.\n",
       "> * **Test Case Prioritization:** Langsmith can help you prioritize test cases based on their criticality and impact on the application, ensuring that the most important tests are executed first.\n",
       "> \n",
       "> **Test Execution and Reporting:**\n",
       "> \n",
       "> * **Test Execution:** Langsmith can automate the execution of test cases, saving time and reducing the risk of human error.\n",
       "> * **Test Reporting:** Langsmith generates detailed test reports that provide insights into the test results, including pass/fail status, execution times, and any errors encountered.\n",
       "> * **Test Analytics:** Langsmith provides analytics and dashboards that help you analyze test results, identify trends, and make informed decisions about your testing strategy.\n",
       "> \n",
       "> **Additional Benefits:**\n",
       "> \n",
       "> * **Improved Code Quality:** Automated testing helps identify defects and bugs early in the development process, leading to higher code quality and reliability.\n",
       "> * **Increased Test Coverage:** Automated testing enables you to cover more test scenarios than manual testing, ensuring that your application is thoroughly tested.\n",
       "> * **Reduced Testing Costs:** Automating testing can significantly reduce the time and resources required for testing, saving you money in the long run.\n",
       "> * **Improved Code Maintainability:** Automated tests serve as documentation for your code, making it easier to understand and maintain in the future."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the LLM directly - there are no chains involved here\n",
    "response = gemini.invoke(\"How can Langsmith help with testing?\")\n",
    "to_markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # NOTE: Gemini does not support system messages, but the parameter\n",
    "        # convert_system_message_to_human=True resolves this for us\n",
    "        (\"system\", \"You are world class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **Langsmith's Capabilities for Testing Assistance**\n",
       "> \n",
       "> As a world-class technical documentation writer, I can leverage Langsmith's capabilities to enhance the testing process:\n",
       "> \n",
       "> **1. Automated Test Case Generation:**\n",
       "> \n",
       "> * Langsmith can automatically generate test cases from documentation, ensuring comprehensive coverage of the system's functionality.\n",
       "> * By analyzing the natural language descriptions in my documentation, Langsmith can identify potential test scenarios and generate corresponding test cases.\n",
       "> \n",
       "> **2. Test Case Management:**\n",
       "> \n",
       "> * Langsmith provides a centralized repository for managing test cases, allowing me to organize, prioritize, and track their execution.\n",
       "> * I can integrate Langsmith with testing frameworks to automate test execution and collect results.\n",
       "> \n",
       "> **3. Test Case Optimization:**\n",
       "> \n",
       "> * Langsmith can analyze test cases and identify redundancies or gaps in coverage.\n",
       "> * It can suggest improvements to optimize test case efficiency and reduce testing time.\n",
       "> \n",
       "> **4. Natural Language Interpretation:**\n",
       "> \n",
       "> * Langsmith's natural language processing capabilities allow it to understand the intent behind my documentation.\n",
       "> * This enables it to generate test cases that accurately reflect the intended behavior of the system.\n",
       "> \n",
       "> **5. Collaboration and Reusability:**\n",
       "> \n",
       "> * Langsmith facilitates collaboration between technical writers and testers.\n",
       "> * Test cases generated from documentation can be easily shared and reused, ensuring consistency and efficiency.\n",
       "> \n",
       "> **Benefits of Using Langsmith for Testing:**\n",
       "> \n",
       "> * **Improved Test Coverage:** Automated test case generation ensures comprehensive testing of system functionality.\n",
       "> * **Reduced Testing Time:** Optimizing test cases and identifying redundancies reduces the time required for testing.\n",
       "> * **Enhanced Accuracy:** Natural language interpretation ensures test cases accurately reflect system behavior.\n",
       "> * **Collaboration and Reusability:** Streamlined collaboration and test case reuse improve efficiency and reduce errors.\n",
       "> * **Integration with Testing Frameworks:** Langsmith's integration capabilities enable seamless execution and reporting of test results.\n",
       "> \n",
       "> By incorporating Langsmith into my technical documentation workflow, I can significantly enhance the testing process, ensuring the quality and reliability of the software systems I document."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain the prompt_template & LLM and make the same query\n",
    "chain = prompt_template | gemini\n",
    "\n",
    "response = chain.invoke({\"input\": \"How can Langsmith help with testing?\"})\n",
    "to_markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an output parser for good measure :)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | gemini | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **LLaMA2: Microsoft's Advanced Language Model**\n",
       "> \n",
       "> LLaMA2 (Large Language Model for Academic Research 2) is a state-of-the-art language model developed by Microsoft Research. It is a transformer-based model with a massive scale of 67 billion parameters, trained on a comprehensive dataset of text and code.\n",
       "> \n",
       "> **Key Features:**\n",
       "> \n",
       "> * **Impressive Language Generation:** LLaMA2 excels in generating coherent and informative text, including stories, articles, and code snippets.\n",
       "> * **Enhanced Reasoning and Inference:** The model demonstrates strong reasoning abilities, enabling it to answer complex questions, draw inferences, and solve problems.\n",
       "> * **Diverse Knowledge Base:** LLaMA2 has been trained on a vast corpus of text, giving it a broad understanding of the world.\n",
       "> * **Code Generation and Analysis:** Unlike other language models, LLaMA2 has been specifically optimized for code generation and analysis tasks.\n",
       "> \n",
       "> **Applications:**\n",
       "> \n",
       "> LLaMA2 has numerous potential applications, including:\n",
       "> \n",
       "> * **Natural Language Processing (NLP):** Text summarization, machine translation, question answering\n",
       "> * **Code Development:** Code completion, code debugging, code generation\n",
       "> * **Education:** Language learning, personalized tutoring\n",
       "> * **Research:** Language modeling, AI development\n",
       "> \n",
       "> **Comparison to Other Models:**\n",
       "> \n",
       "> Compared to other language models such as GPT-3 and BLOOM, LLaMA2 demonstrates:\n",
       "> \n",
       "> * **Improved Reasoning:** LLaMA2 shows enhanced reasoning capabilities, leading to more accurate and insightful answers.\n",
       "> * **Code-Centric Focus:** LLaMA2's specific training on code enables it to generate and analyze code with high accuracy.\n",
       "> * **Open Access:** While other models may be restricted in their availability, LLaMA2 is open for academic research and exploration.\n",
       "> \n",
       "> **Availability:**\n",
       "> \n",
       "> LLaMA2 is currently available through a limited access program for researchers and developers. Microsoft plans to release the model more widely in the future.\n",
       "> \n",
       "> **Conclusion:**\n",
       "> \n",
       "> LLaMA2 is a groundbreaking language model that pushes the boundaries of AI capabilities. With its advanced reasoning, impressive text generation, and code-centric focus, it holds immense potential for revolutionizing various fields and applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(chain.invoke({\"input\": \"Tell me about LLama2\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Retrieval Chain with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# to enable a more contextual search, we point to the online documentation\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings for the web page\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "# create the vector store (NOTE: this will be re-built every time & lost on exit)\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have this data indexed in a vectorstore, we will create a retrieval chain.\n",
    "# This chain will take an incoming question, look up relevant documents, then pass those\n",
    "# documents along with the original question into an LLM and ask it to answer the original question\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context. If you cannot find the answer,\n",
    "       respond with \"Aplogies, but I cannot find the answer in the context.\" Dont assume things.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(gemini, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a retrieval chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplogies, but I cannot find the answer in the context.\n"
     ]
    }
   ],
   "source": [
    "# now let's try something that will fail\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with deployments?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Opensource LLMs (LLama2) with Ollama\n",
    "Please note that since your LLM is running locally, performance depends on your hardware. Generally the more the memory (and more the GPU memory) you have, the faster will be the response. Response will be slowest from a CPU only machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llama2 = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama2.invoke(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a teacher. Respond as though you are teaching 7 year olds\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWow, that's a great question! *excited face* The capital of Australia is Canberra! *points on a map* It's a beautiful city with lots of fun things to see and do. Have you ever been there before? *smiling* Let me tell you more about it! ðŸ˜Š\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = prompt_template2 | llama2\n",
    "\n",
    "chain2.invoke(input={\"question\": \"What is the capital of Australia?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.13'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
